{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 기초 수학"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 벡터는 뭔가요?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 열벡터는 `list`, 행벡터는 `np.array`로 표현\n",
    "- 벡터는 공간에서 `한 점`을 나타냄.\n",
    "- 벡터의 `노름(norm)`은 원점에서부터의 거리\n",
    "  - L1 노름은 각 성분의 `변화량의 절대값`을 모두 더함\n",
    "  - L2 노름은 피타고라스 정리를 이용해 `유클리드 거리`를 계산함\n",
    "    - `np.linalg.norm`으로 구현 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 내적\n",
    "  - 내적은 `정사영된 벡터의 길이`와 관련이 있다\n",
    "  - Proj(x)의 길이는 `코사인법칙`에의해 $||x||\\cos{\\theta}$가 된다\n",
    "  - 내적은 정사영의 길이를 `벡터 y의 길이 `$||y||$`만큼 조정`한 값이다\n",
    "  \n",
    "<img src=\"./img/내적.JPG\" width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 행렬은 뭔가요?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 행렬(matrix)은 벡터를 원소로 가지는 `2차원 배열`\n",
    "- 행렬 곱셈은 `i번째 행벡터와 j번째 열벡터 사이의 내적`을 성분으로 가지는 행렬을 계산함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[1, -2, 3],\n",
    "              [7,5,0],\n",
    "              [-2,-1,2]])\n",
    "y = np.array([[0,1],\n",
    "              [1,-1],\n",
    "              [-2,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-8,  6],\n",
       "       [ 5,  2],\n",
       "       [-5,  1]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x @ y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-8,  6],\n",
       "       [ 5,  2],\n",
       "       [-5,  1]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 넘파이의 `np.inner`는 `i번째 행벡터와 j번째 행벡터 사이의 내적`을 성분으로 가지는 행렬을 계산함.\n",
    "- 수학에서 말하는 내적과는 다르므로 주의\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 행렬은 `벡터공간에서 사용되는 연산자`로 이해한다.\n",
    "- 행렬곱을 통해 벡터를 `다른 차원의 공간`으로 보낼 수 있다.\n",
    "- 행렬곱을 통해 `패턴을 추출`할 수 있고 `데이터를 압축`할 수도 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 역행렬\n",
    "  - 어떤 행렬 A의 연산을 거꾸로 되돌리는 행렬을 `역행렬`이라고 부르고 $A^{-1}$라 표기한다. 역행렬은 `행과 열 숫자가 같고 행렬식이 0이 아닌 경우`에만 계산할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.21276596,  0.0212766 , -0.31914894],\n",
       "       [-0.29787234,  0.17021277,  0.44680851],\n",
       "       [ 0.06382979,  0.10638298,  0.40425532]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 행과 열의 숫자가 같고 행렬식이 0이 아닌 행렬만 역행렬을 구할 수 있음\n",
    "x = np.array([[1, -2, 3],\n",
    "              [7,5,0],\n",
    "              [-2,-1,2]])\n",
    "np.linalg.inv(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 만일 역행렬을 계산할 수 없다면 `유사역행렬(pseudo-inverse)`또는 `무어-펜로즈(Moore-Penrose) 역행렬` $A^+$을 이용한다\n",
    "  - 행의 개수와 열의 개수 중 무엇이 더 큰 지에 따라 계산식이 달라짐\n",
    "  \n",
    "  <img src=\"./img/유사역행렬.JPG\" width=400>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.00000000e-01,  1.11022302e-16, -5.00000000e-01],\n",
       "       [ 8.33333333e-01, -3.33333333e-01, -1.66666667e-01]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[0, 1],\n",
    "             [1, -1],\n",
    "             [-2, 1]])\n",
    "np.linalg.pinv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.00000000e+00, -2.22044605e-16],\n",
       "       [ 1.11022302e-16,  1.00000000e+00]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.pinv(x) @ x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 확률론 맛보기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 딥러닝은 `확률론 기반의 기계학습 이론`에 바탕을 두고 있다.\n",
    "- 기계학습에서 사용되는 손실함수(loss function)들의 작동 원리는 데이터 공간을 통계적으로 해석해서 유도하게 된다.\n",
    "- 회귀분석에서 손실함수로 사용되는 $L_2$-노름은 `예측오차의 분산을 가장 최소화하는 방향으로 학습`하도록 유도한다.\n",
    "- 분류 문제에서 사용되는 교차 엔트로피(cross-entropy)는 `모델 예측의 불확실성을 최소화하는 방향으로 학습`하도록 유도한다.\n",
    "- 분산 및 불확실성을 `최소화하기 위해서는 측정하는 방법`을 알아야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 조건부확률 $P(y|x)$는 입력변수 x에 대해 정답이 y일 확률을 의미한다.\n",
    "- 로지스틱 회귀에서 사용했던 선형모델과 소프트맥스 함수의 결합은 `데이터에서 추출된 패턴을 기반으로 확률을 해석`하는데 사용된다.\n",
    "- 분류 문제에서 softmax($W\\phi + b$)은 데이터 x로부터 추출된 특징패턴 $\\phi(x)$과 가중치행렬 W을 통해 조건부확률$P(y|x)$을 계산한다.\n",
    "- 회귀 문제의 경우 조건부기대값 $E[y|x]$을 추정한다.\n",
    "- 딥러닝은 다층신경망을 사용하여 데이터로부터 특징패턴 $\\phi$를 추출한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 몬테카를로 샘플링\n",
    "  - 기계학습의 많은 문제들은 확률분포를 명시적으로 모를 때가 대부분이다.\n",
    "  - 확률분포를 모를 때 `데이터를 이용하여 기대값을 계산하려면 몬테카를로(Monte Carlo) 샘플링 방법을 사용`해야 한다.\n",
    "  \n",
    "  <img src='./img/몬테카를로식.JPG' height=90>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 통계학 맛보기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `통계적 모델링은 적절한 가정 위에서 확률분포를 추정(inference)`하는 것이 목표이며, 기계학습과 통계학이 공통적으로 추구하는 목표이다.\n",
    "- 그러나 유한한 개수의 데이터만 관찰해서 모집단의 분포를 정확하게 알아낸다는 것은 불가능하므로, `근사적으로 확률분포를 추정`할 수 밖에 없다.\n",
    "- `모수적(parametric) 방법론` : 데이터가 특정 확률분포를 따른다고 선험적으로 가정한 후 그 분포를 결정하는 모수(parameter)를 추정하는 방법론\n",
    "- `비모수적(nonparametric) 방법론` : 특정 확률분포를 가정하지 않고 데이터에 따라 모델의 구조 및 모수의 개수가 유연하게 바뀌는 방법론"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 최대가능도 추정법\n",
    "  - 표본평균이나 표본분산은 중요한 통게량이지만 확률분포마다 사용하는 모수가 다르므로 적절한 통계량이 달라지게 된다.\n",
    "  - 이론적으로 가장 가능성이 높은 모수를 추정하는 방법 중 하나는 `최대가능도추정법(maximum likelihood estimation, MLE)`이다.\n",
    "\n",
    "  <img src='./img/MLE.PNG' height=80>\n",
    "\n",
    "  - 데이터 집합 X가 `독립적으로 추출되었을 경우 로그가능도를 최적화`한다.\n",
    "\n",
    "  <img src='./img/로그가능도최적화.PNG' height=80>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **왜 로그가능도를 사용하나요?**\n",
    "  - 로그가능도는 최적화하는 모수 $\\theta$는 가능도를 최적화하는 MLE가 된다.\n",
    "  - 데이터의 숫자가 적으면 상관없지만 `만일 데이터의 숫자가 수억 단위가 된다면 컴퓨터의 정확도로는 가능도를 계산하는 것은 불가능`\n",
    "  - 데이터가 독립일 경우 로그를 사용하면 가능도의 곱셈을 로그가능도의 덧셈으로 바꿀 수 있기 때문에 컴퓨터로 연산이 가능해짐.\n",
    "  - 경사하강법으로 가능도를 최적화할 때 미분 연산을 사용하게 되는데, 로그가능도를 사용하면 `연산량을 `$O(n^2)$`에서 `$O(n)$`으로 줄여준다.`\n",
    "  - 대개의 손실함수의 경우 경사하강법을 사용하므로 `음의 로그가능도(negative log-likelihood)를 최적화`하게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 딥러닝에서 최대가능도 추정법\n",
    "  - 최대가능도 추정법을 이용해서 기계학습 모델을 학습할 수 있다.\n",
    "  - 딥러닝 모델의 가중치를 $\\theta = (W^{(1)}, ... , W^{(L)})$라 표기했을 때 분류 문제에서 소프트맥스 벡터는 카테고리분포의 모수 $(p_1, ... , p_K)$를 모델링한다.\n",
    "  - 원핫벡터로 표현한 정답레이블 $y = (y_1, ..., y_K)$을 관찰데이터로 이용해 확률분포인 소프트맥스 벡터의 로그가능도를 최적화할 수 있다.\n",
    "\n",
    "  <img src='./img/세타햇MLE.PNG' height=80>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 확률분포의 거리를 구해보자\n",
    "  - 기계학습에서 사용되는 손실함수들은 모델이 학습하는 확률분포와 데이터에서 관찰되는 확률분포의 거리를 통해 유도한다.\n",
    "  - 데이터공간에 두 개의 확률분포 P(x), Q(x)가 있을 경우 `두 확률분포 사이의 거리(distance)를 계산`할 때 다음과 같은 함수들을 이용한다.\n",
    "    - 총변동 거리 (Total Variation Distance, TV)\n",
    "    - 쿨백-라이블러 발산 (Kullback-Leibler Divergence, KL)\n",
    "    - 바슈타인 거리 (Wasserstein Distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 쿨백-라이블러 발산 (KL 발산)\n",
    "  - KL 발산은 다음과 같이 정의한다.\n",
    "\n",
    "  <img src='./img/KL발산.PNG' height=90>\n",
    "\n",
    "  - KL 발산은 다음과 같이 분해할 수 있다.\n",
    "\n",
    "  <img src='./img/KL발산분해.PNG' height=90>\n",
    "\n",
    "  - 분류 문제에서 정답레이블을 P, 모델 예측을 Q라 두면 `최대가능도 추정법은 KL발산을 최소화`하는 것과 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 베이즈 통계학 맛보기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 영상 봐라."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 경사하강법(순한맛)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 미분 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\operatorname{Poly}{\\left( 2 x + 2, x, domain=\\mathbb{Z} \\right)}$"
      ],
      "text/plain": [
       "Poly(2*x + 2, x, domain='ZZ')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sympy as sym\n",
    "from sympy.abc import x\n",
    "\n",
    "sym.diff(sym.poly(x**2 + 2*x + 3), x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 변수가 벡터일 때\n",
    "  - 미분은 `변수의 움직임에 따른 함수값의 변화를 측정하기 위한 도구`로 최적화에서 제일 많이 사용하는 기법이다.\n",
    "  - 벡터가 입력값인 다변수 함수의 경우 `편미분(partial differentiation)`을 사용한다.\n",
    "\n",
    "  <img src=\"./img/편미분.PNG\" height=90>\n",
    "\n",
    "  - 각 변수별로 편미분을 계산한 `그레디언트(gradient) 벡터`를 이용하여 경사하강/경사상승법에 사용할 수 있다.\n",
    "\n",
    "  <img src=\"./img/nablaf.PNG\" height=120>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 2 x + 2 y - \\sin{\\left(x + 2 y \\right)}$"
      ],
      "text/plain": [
       "2*x + 2*y - sin(x + 2*y)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "from sympy.abc import y\n",
    "\n",
    "sym.diff(sym.poly(x**2 + 2*x*y + 3) + sym.cos(x + 2*y), x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 경사하강법(매운맛)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 앞서 행렬파트에서 무어펜로즈 역행렬을 이용해 `선형회귀식`을 구했었는데, 이번에는 `경사하강법`을 이용해서 적절한 선형모델을 찾아본다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 선형회귀의 목적식은 $||y-X\\beta||_2$이고 이를 최소화하는 $\\beta$를 찾아야 하므로 다음과 같은 그레디언트 벡터를 구해야 한다.\n",
    "\n",
    "<img src=\"./img/제곱없이.PNG\" height=120>\n",
    "\n",
    "- 이제 목적식을 최소화하는 $\\beta$를 구하는 경사하강법 알고리즘은 다음과 같다.\n",
    "\n",
    "<img src=\"./img/제곱없이경사하강법.PNG\" height=120>\n",
    "\n",
    "- 여기서 $||y-X\\beta||_2$ 대신 $||y-X\\beta||_2^2$를 최소화하면 식이 좀 더 간단해진다.\n",
    "\n",
    "<img src=\"./img/제곱하고.PNG\" height=120>\n",
    "\n",
    "<img src=\"./img/제곱하고경사하강법.PNG\" height=120>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('da')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cc559ded4c07dd5d7e91d31f6492afc13b9aee8598d231e373ec0f74b4653a08"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
