{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 머신러닝의 학습 방법들\n",
    "- Gradient descent based learning\n",
    "    - 실제 값과 학습된 모델 예측치의 오차를 최소화하는 방법\n",
    "    - 모델의 최적 parameter 찾기가 목적\n",
    "    - 오차의 합이 양수가 될 수도 음수가 될 수도 있어서 상쇄될 수 있음\n",
    "    - 그렇기 때문에 제곱의 합으로 변환하여 오차를 계산함(Squared Error)\n",
    "    - Squared Error를 최소화할 수 있는 weight값을 발견해야 함\n",
    "        - 최소 또는 최대의 문제이기 때문에 미분으로 해결할 수 있음\n",
    "- Probability theory based learning\n",
    "- Information theory based learning\n",
    "- Distance similarity based learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 가설함수\n",
    "- $f(x) = h_{\\theta}(x)$\n",
    "    - 앞으로 우리는 예측함수를 가설함수라고 부를 예정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost function\n",
    "- $J(w_0, w_1) = {1\\over2m}\\displaystyle\\sum_{i=0}^{m}{(h_{\\theta}(x^{(i)})-y^{(i)})^2}$\n",
    "    - 실제값과 가설함수의 차이\n",
    "    - Cost function의 최소화를 위한 weight값을 구해야 함\n",
    "    - $J(w_0, w_1)$를 $w_0, w_1$로 편미분해서 0이 되는 값을 찾기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weights의 최적값을 컴퓨터가 찾는 방법\n",
    "- 연립방정식 풀기(normal equation)\n",
    "- gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal Equation\n",
    "- Cost function을 편미분해서 0이 되는 값 찾기\n",
    "> $J = {1\\over2m}\\displaystyle\\sum_{i=0}^{m}{(h_{\\theta}(x^{(i)})-y^{(i)})^2}$ <br>\n",
    "> ${{\\partial}J\\over{\\partial}{w_0}} = \\sum{({{w_1}x^{(i)}}+w_0-y^{(i)})} = 0$ <br>\n",
    "> ${{\\partial}J\\over{\\partial}{w_1}} = \\sum{({{w_1}x^{(i)}}+w_0-y^{(i)})x^{(i)}} = 0$\n",
    "- 위 식을 만족하는 $\\hat{w}_j$값을 구하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 각 식을 풀어서 다시 쓰기\n",
    "> $\\hat{w}_0m + \\hat{w}_1\\sum{x^{(i)}} = \\sum{y^{(i)}}$ <br>\n",
    "> $\\hat{w}_0\\sum{x^{(i)}} + \\hat{w}_1\\sum{(x^{(i)})^2} = \\sum{y^{(i)}x^{(i)}}$ <br>\n",
    "\n",
    "> $X = \\begin{bmatrix} 1 & x^{(1)} \\\\ 1 & x^{(2)} \\\\ 1 & x^{(3)} \\\\ 1 & x^{(4)} \\\\ 1 & x^{(5)} \\end{bmatrix}$, \n",
    "> $\\hat{w} = \\begin{bmatrix} w_0 \\\\ w_1 \\end{bmatrix}$, \n",
    "> $y = \\begin{bmatrix} y^{(1)} \\\\ y^{(2)} \\\\ y^{(3)} \\\\ y^{(4)} \\\\ y^{(5)} \\end{bmatrix}$\n",
    "\n",
    "> $X^TX = \\begin{bmatrix} m & \\sum{x^{(i)}} \\\\ \\sum{x^{(i)}} & \\sum{(x^{(i)})^2} \\end{bmatrix}$ <br>\n",
    "- 아래 식은 위에서 다시 쓴 2개의 편미분 식과 같음\n",
    "> $(X^TX)\\hat{w} = X^Ty$ <br>\n",
    "> $\\hat{w} = (X^TX)^{-1}X^Ty$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $x^{(i)}$의 합을 $m\\bar{x}$로 표현\n",
    "> $X^TX = \\begin{bmatrix} m & \\sum{x^{(i)}} \\\\ \\sum{x^{(i)}} & \\sum{(x^{(i)})^2} \\end{bmatrix}$ = $\\begin{bmatrix} m & m\\bar{x} \\\\ m\\bar{x} & \\sum{(x^{(i)})^2} \\end{bmatrix}$ <br>\n",
    "- $|X^TX|$ 는 곧 분산을 의미함\n",
    "> $|X^TX| = m\\sum{(x^{(i)})^2 - (m\\bar{x})^2} = m(\\sum{(x^{(i)})^2 - m\\bar{x}^2}) = m\\sum{(x^{(i)} - \\bar{x})^2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 행렬식을 구해서 역행렬 구하기\n",
    "> $(X^TX)^{-1} = {{1}\\over{m\\sum{(x^{(i)}} - \\bar{x})^2}}\\begin{bmatrix} \\sum{(x^{(i)})^2} & -m\\bar{x} \\\\ -m\\bar{x} & m \\end{bmatrix} = {{1}\\over{\\sum{(x^{(i)}} - \\bar{x})^2}}\\begin{bmatrix} \\sum{(x^{(i)})^2/m} & -\\bar{x} \\\\ -\\bar{x} & 1 \\end{bmatrix}$ <br>\n",
    "\n",
    "> $X^Ty = \\begin{bmatrix} \\sum{y^{(i)}} \\\\ \\sum{x^{(i)}y^{(i)}} \\end{bmatrix}, X = \\begin{bmatrix} 1 & x^{(1)} \\\\ 1 & x^{(2)} \\\\ 1 & x^{(3)} \\\\ 1 & x^{(4)} \\\\ 1 & x^{(5)} \\end{bmatrix}, y = \\begin{bmatrix} y^{(1)} \\\\ y^{(2)} \\\\ y^{(3)} \\\\ y^{(4)} \\\\ y^{(5)} \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 결국 우리가 찾고자 하는 것은 $\\hat{w}_0, \\hat{w}_1$\n",
    "> $\\hat{w} = (X^TX)^{-1}X^Ty = \\begin{bmatrix} \\hat{w}_0 \\\\ \\hat{w}_1 \\end{bmatrix} = {{1}\\over{\\sum{(x^{(i)}} - \\bar{x})^2}}\\begin{bmatrix} \\sum{(x^{(i)})^2/m} & -\\bar{x} \\\\ -\\bar{x} & 1 \\end{bmatrix}\\begin{bmatrix} \\sum{y^{(i)}} \\\\ \\sum{x^{(i)}y^{(i)}} \\end{bmatrix} $ <br>\n",
    "\n",
    ">> $\\hat{w}_0 = \\bar{y} - \\hat{w}_1\\bar{x}$ <br>\n",
    ">> $\\hat{w}_1 = \\frac{\\sum{x^{(i)}y^{(i)}} - m\\bar{x}\\bar{y}}{\\sum{(x^{(i)} - \\bar{x})^2}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 여러 개의 변수일 경우 :\n",
    "> $X^TX = \\begin{bmatrix} m & m\\bar{x} \\\\ m\\bar{x} & \\sum{(x^{(i)})^2} \\end{bmatrix}$ 가 확대됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal Equation의 결론\n",
    "> ***$\\hat{w} = (X^TX)^{-1}X^Ty$***\n",
    "- $X^TX$의 역행렬이 존재할 때 사용\n",
    "- Iteration 등 사용자 지정 parameter가 없음\n",
    "- Feature가 많으면 계산 속도가 느려짐"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('da')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cc559ded4c07dd5d7e91d31f6492afc13b9aee8598d231e373ec0f74b4653a08"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
