{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfitting and Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting\n",
    "- 모델이 Training data에 과다하게 최적화되어 새로운 데이터의 예측력이 떨어지는 현상"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias-Variance tradeoff\n",
    "- High bias : underfitting. 높은 편향. 원래 모델에 많이 떨어짐.\n",
    "    - 잘못된 데이터만 계속 학습함\n",
    "    - 잘못된 weight만 update\n",
    "- High variance : overfitting. 높은 분산. 모든 데이터에 민감하게 학습.\n",
    "    - error를 고려하지 않음\n",
    "    - 모든 weight가 update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overcoming Overfitting\n",
    "- 더 많은 데이터를 활용한다.\n",
    "    - 가장 좋은 방법이지만 현실적으로 불가능한 경우가 많음.\n",
    "- Feature의 개수를 줄인다.\n",
    "- 적절히 parameter를 선정한다.\n",
    "- Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "- weight에 penalty를 부과하여 overfitting을 방지하는 것."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $J(w_0, w_1) = {1\\over2m}\\displaystyle\\sum_{i=1}^{m}{(w_1x^{(i)} + w_0 - y^{(i)})^2}$\n",
    "- 위의 cost function에 $1000w_1$을 추가하여\n",
    "\n",
    "> $J(w_0, w_1) = {1\\over2m}\\displaystyle\\sum_{i=1}^{m}{(w_1x^{(i)} + w_0 - y^{(i)})^2} + 1000w_1$\n",
    "- 위의 cost function을 최소화하는 $w_1$를 찾기 위해서는, $w_1$가 조금만 커져도 $J$가 많이 커지기 때문에 $w_1$가 작아질 수 밖에 없을 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L2 Regularization\n",
    "- 기존 Cost function에 L2(norm) penalty term을 추가\n",
    "\n",
    "> $J(\\theta) = {1\\over2m}\\displaystyle\\sum_{i=1}^{m}{(h_{\\theta}(x^{(i)})-y^{(i)})^2} + \\frac{\\lambda}{2}\\displaystyle\\sum_{j=1}^{m}{{\\theta_j}^2}$\n",
    "\n",
    "- norm : 벡터의 길이 혹은 크기를 측정하는 방법\n",
    "    - L2는 Euclidean distance (원점에서 벡터 좌표까지의 거리)\n",
    "\n",
    "> ${||(\\theta)||_2^2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 식 전개\n",
    "\n",
    "> $J(\\theta) = {1\\over2m}\\displaystyle\\sum_{i=1}^{m}{(h_{\\theta}(x^{(i)})-y^{(i)})^2} + \\frac{\\lambda}{2}\\displaystyle\\sum_{j=1}^{m}{{\\theta_j}^2}$ <br>\n",
    "> $\\theta_0 := \\theta_0 - \\alpha\\frac{1}{m}\\displaystyle\\sum_{i=1}^{m}{(h_{\\theta}(x^{(i)})-y^{(i)}){x_0}^{(i)}}$ <br>\n",
    "> $\\theta_j := \\theta_j - \\alpha[(\\frac{1}{m}\\displaystyle\\sum_{i=1}^{m}{(h_{\\theta}(x^{(i)})-y^{(i)}){x_j}^{(i)}}) + \\frac{\\lambda}{m}\\theta_j]$ <br>\n",
    "> $\\theta_j := \\theta_j(1-\\alpha\\frac{\\lambda}{m}) - \\alpha\\frac{1}{m}\\displaystyle\\sum_{i=1}^{m}{(h_{\\theta}(x^{(i)})-y^{(i)}){x_j}^{(i)}}$\n",
    "\n",
    "- 마지막 식에서 $\\lambda$에 의해 $(1-\\alpha\\frac{\\lambda}{m})$의 크기가 정해진다.\n",
    "- 이때, $(1-\\alpha\\frac{\\lambda}{m})$는 0과 1 사이의 값을 가질 수 밖에 없으므로 $\\lambda$에 의해 $\\theta_j$가 작아지는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal Equation Approach\n",
    "> $J(\\theta) = {1\\over2m}\\displaystyle\\sum_{i=1}^{m}{(h_{\\theta}(x^{(i)})-y^{(i)})^2} + \\frac{\\lambda}{2}\\displaystyle\\sum_{j=1}^{m}{{\\theta_j}^2}$ <br>\n",
    "\n",
    "> $J(\\theta)$ <br>\n",
    "> $= (y - X\\theta)^T(y - X\\theta) + \\lambda\\theta^T\\theta$ <br>\n",
    "> $= y^Ty - \\theta^TX^Ty - y^TX\\theta + \\theta^TX^TX\\theta + \\lambda\\theta^T\\theta$ <br>\n",
    "> $= y^Ty - \\theta^TX^Ty - \\theta^TX^Ty + \\theta^TX^TX\\theta + \\theta^T{\\lambda}I\\theta$ <br>\n",
    "> $= y^Ty - 2\\theta^TX^Ty + \\theta^T(X^TX + {\\lambda}I)\\theta$ <br>\n",
    "\n",
    "> $\\frac{{\\partial}J(\\theta)}{{\\partial}\\theta} = -2X^Ty + 2(X^TX + {\\lambda}I)\\theta$ <br>\n",
    "> $(X^TX + {\\lambda}I)\\theta = X^Ty \\Rightarrow \\hat{\\theta} = (X^TX + {\\lambda}I)^{-1}X^Ty$\n",
    "\n",
    "- 위의 2번째와 3번째 전개식 중에서 $y^TX\\theta$ -> $\\theta^TX^Ty$의 변환이 가능한 이유는 $X\\theta$와 $y$가 모두 vector이기 때문임."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L1 Regularization\n",
    "- 기존 Cost function에 L1(norm) penalty term을 추가\n",
    "> $J(\\theta) = {1\\over2m}\\displaystyle\\sum_{i=1}^{m}{(h_{\\theta}(x^{(i)})-y^{(i)})^2} + \\frac{\\lambda}{2}\\displaystyle\\sum_{j=1}^{m}{|\\theta_j|}$ <br>\n",
    "\n",
    "- norm : 벡터의 길이 혹은 크기를 측정하는 방법\n",
    "    - L1은 Manhattan distance (원점에서 벡터 좌표까지의 거리)\n",
    "\n",
    "> ${||x||}_1 := \\sum_{n}^{i=1}{|x_i|}$\n",
    "\n",
    "- L1 정규화보단 L2 정규화를 많이 쓰기 때문에 자세한 설명은 생략"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L1 vs L2\n",
    "| L1 | L2 |\n",
    "|---|---|\n",
    "| Unstable solution | Stable solution |\n",
    "| One or More solution | Only one solution |\n",
    "| Sparse solution | Non-sparse solution |\n",
    "| Feature selection |  |\n",
    "- `stable` : L1의 경우 절댓값이기 때문에 미분할 때 어려움이 있다. L2는 그에 비해 비교적 안정적이다.\n",
    "- `Sparse` : L1의 경우 weight값이 0이 되는 경우가 생길 확률이 있음.\n",
    "  - 이 때 weight값이 0이 된다는 것은 해당 feature가 중요도가 낮다는 것을 의미하고, 그렇게 feature를 고를 수 있게 된다.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
